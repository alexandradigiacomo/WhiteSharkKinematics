{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1XrZSkyG7Rt",
        "outputId": "b602cb9a-b576-4e03-99a5-0e0963ec36aa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_csv(file):\n",
        "    \"\"\"Modifies row/column structure of an input DLC coordinate CSV\"\"\"\n",
        "    df = pd.read_csv(file, encoding='utf-8')\n",
        "\n",
        "    # extract the first row (body parts) and second row (coordinate labels)\n",
        "    first_row, second_row = df.iloc[0, 1:].values, df.iloc[1, 1:].values\n",
        "\n",
        "    # create new column names\n",
        "    new_columns = [f\"{first_row[i]}_{second_row[i]}\" for i in range(len(first_row))]\n",
        "    new_columns = ['index'] + new_columns\n",
        "    df.columns = new_columns\n",
        "\n",
        "    # drop original first two rows\n",
        "    df = df.drop([0, 1]).reset_index(drop=True)\n",
        "\n",
        "    # drop rows in which all cell values are zero except index\n",
        "    df = df.astype(float)\n",
        "    df = df.loc[~(df.iloc[:, 1:].eq(0)).all(axis=1)]\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [],
      "source": [
        "def correct_angle(angle):\n",
        "    \"\"\" Adjusts angle values so that angle > pi is wrapped back into the range -pi to pi\"\"\"\n",
        "    thresh = np.pi\n",
        "    ang = np.where(angle > thresh, angle - 2 * thresh, np.where(angle < -thresh, angle + 2 * thresh, angle))\n",
        "\n",
        "    return ang"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [],
      "source": [
        "def frame_cleaner(df, low_likelihood_thresh=0.9, frame_len_thresh=120, low_likelihood_frame_len=10, crit_low_likelihood_thresh=0.1):\n",
        "  \"\"\"\n",
        "  Cleans DLC coordinate CSVs to a specified length and splits sequences with repeatedly low likelihoods\n",
        "    Args: \n",
        "      DLC coordinate file processed by process_csv()\n",
        "      frame_len_thresh (int): minimum # frames for a valid sequence\n",
        "      low_likelihood_thresh (float): threshold by which to consider a frame 'low likelihood'\n",
        "      low_likelihood_frame_len (int): # consecutive low-likelihood frames required to drop a section\n",
        "      crit_low_likelihood_thresh (float): criticially low-likelihood threshold to drop frames entirely\n",
        "    Returns: cleaned dataframe\n",
        "  \"\"\"\n",
        "  # set critically low likelihood points to NaN\n",
        "  df.loc[df['nose_likelihood'] < crit_low_likelihood_thresh, \n",
        "          ['nose_x', 'nose_y', 'dorsal_x', 'dorsal_y', 'caudal_x', 'caudal_y']] = np.nan\n",
        "  \n",
        "  valid_data = []  # valid segments\n",
        "  segment_id = 0   \n",
        "  buffer = []      # temp storage for the current valid segment\n",
        "  counter = 0      # consecutive low-likelihood frames\n",
        "  in_bad_segment = False  # if in a low-likelihood section\n",
        "\n",
        "  for index, row in df.iterrows():\n",
        "      # check if row bad (either all NaN for body parts OR nose_likelihood < threshold)\n",
        "      is_bad_row = row[['nose_x', 'nose_y', 'dorsal_x', 'dorsal_y', 'caudal_x', 'caudal_y']].isna().all() or row['nose_likelihood'] < low_likelihood_thresh\n",
        "\n",
        "      if is_bad_row:\n",
        "        counter += 1  # increase consecutive counter\n",
        "        if counter >= low_likelihood_frame_len and not in_bad_segment:\n",
        "            in_bad_segment = True  # turn bad segment pointer on\n",
        "\n",
        "            # save current buffer as a segment if it meets the frame len thresh\n",
        "            if len(buffer) >= frame_len_thresh:\n",
        "                valid_segment = pd.DataFrame(buffer)\n",
        "                valid_segment['segment'] = segment_id\n",
        "                valid_data.append(valid_segment)\n",
        "                segment_id += 1  # increment segment id\n",
        "\n",
        "            buffer = []  # reset buffer\n",
        "      else:\n",
        "        counter = 0  # reset counter, we found a good frame\n",
        "        in_bad_segment = False  # mark that we're in a valid segment\n",
        "        row['index'] = index # track index\n",
        "        buffer.append(row)  # store row in buffer\n",
        "\n",
        "  # store the last valid segment if it's long enough\n",
        "  if len(buffer) >= frame_len_thresh:\n",
        "      valid_segment = pd.DataFrame(buffer)\n",
        "      valid_segment['segment'] = segment_id\n",
        "      valid_data.append(valid_segment)\n",
        "\n",
        "  # Combine valid segments into a single dataframe\n",
        "  if valid_data:\n",
        "      cleaned_df = pd.concat(valid_data).reset_index(drop=True)\n",
        "      return cleaned_df\n",
        "  else:\n",
        "      return pd.DataFrame() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "vJ1va86H_3vD"
      },
      "outputs": [],
      "source": [
        "def add_angles(df,tb_duration=60):\n",
        "  \"\"\" computes body angles while ensuring consistency of left/right\n",
        "  Args: \n",
        "    df: dataframe pre-processed by frame_cleaner(process_csv())\n",
        "    tb_duration (int): approximate # frames of a tb cycle used to smooth nd vector & define head wagging effect\n",
        "      default is 60 frames (0.4hz)\n",
        "  Returns: df with body angles (theta_head, theta_flex)\n",
        "    theta_head: angle of the nd vector relative to a smoothed nd vector over the course of a tb (approx. degrees head wagging)\n",
        "    theta_flex: angle of flexion of the body, instantaneous nd to dc\n",
        "  \"\"\"\n",
        "  # vectors\n",
        "  nd_x = df['dorsalfin_x'] - df['nose_x']\n",
        "  nd_y = df['dorsalfin_y'] - df['nose_y']\n",
        "  dc_x = df['caudalfin_x'] - df['dorsalfin_x']\n",
        "  dc_y = df['caudalfin_y'] - df['dorsalfin_y']\n",
        "\n",
        "  # smooth nd (heading) vector\n",
        "  nd_x_smooth = nd_x.rolling(window=int(tb_duration), center=True, min_periods=1).mean()\n",
        "  nd_y_smooth = nd_y.rolling(window=int(tb_duration), center=True, min_periods=1).mean()\n",
        "\n",
        "  # vector angles\n",
        "  ang_nd = np.arctan2(nd_x, nd_y)\n",
        "  ang_nd_smooth = np.arctan2(nd_x_smooth, nd_y_smooth)\n",
        "  ang_dc = np.arctan2(dc_x, dc_y)\n",
        "\n",
        "  # angles theta_head and theta_flex\n",
        "  theta_head = ang_nd_smooth - ang_nd\n",
        "  theta_flex = ang_nd - ang_dc ###################### change for relative vs. instantaneous #########\n",
        "\n",
        "  # correct and convert\n",
        "  df['theta_head_deg'] = np.degrees(correct_angle(theta_head))\n",
        "  df['theta_flex_deg'] = np.degrees(correct_angle(theta_flex))\n",
        "\n",
        "  # add lengths (pixels)\n",
        "  nd_length = np.sqrt(nd_x**2 + nd_y**2)  # nd\n",
        "  dc_length = np.sqrt(dc_x**2 + dc_y**2)  # dc\n",
        "  BL = nd_length + dc_length # BL\n",
        "  df['BL_pixels']  = BL\n",
        "\n",
        "  # add instantaneous flexion and head amplitudes (body lengths)\n",
        "  df['amp_flex_BL'] = np.abs(np.sin(np.radians(df['theta_flex_deg'])) * dc_length) / BL\n",
        "  df['amp_head_BL'] = np.abs(np.sin(np.radians(df['theta_head_deg'])) * nd_length) / BL\n",
        "\n",
        "  return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_angles(df, bio_threshold_flex=75, bio_threshold_head=60):\n",
        "  \"\"\" Cleans tail angle series with biological threshold and z-score filter. \n",
        "  Args: \n",
        "    df: a DataFrame containing tail angles from add_angles()\n",
        "    bio_threshold_head (float): threshold of head wag (average nd- instaneous nd) beyond which would be biologically infeasible\n",
        "    bio_threshold_flex (float): threshold of tail flexion (nd-dc) beyond which would be biologically infeasible\n",
        "  Returns: a cleaned DataFrame\n",
        "  \"\"\"\n",
        "  # z-score filtering (head angle)\n",
        "  Q1, Q3 = df['theta_head_deg'].quantile(0.25), df['theta_head_deg'].quantile(0.75)\n",
        "  IQR = Q3 - Q1\n",
        "  lower_bound, upper_bound = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
        "\n",
        "  # threshold (head angle)\n",
        "  for i, angle in enumerate(df['theta_head_deg']):\n",
        "    if angle > bio_threshold_head or angle < -bio_threshold_head:\n",
        "      df.at[i, 'theta_head_deg'] = np.nan\n",
        "    if angle <= lower_bound or angle >= upper_bound:\n",
        "      df.at[i, 'theta_head_deg'] = np.nan\n",
        "\n",
        "  # z-score filtering (flex angle)\n",
        "  Q1, Q3 = df['theta_flex_deg'].quantile(0.25), df['theta_flex_deg'].quantile(0.75)\n",
        "  IQR = Q3 - Q1\n",
        "  lower_bound, upper_bound = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
        "\n",
        "  # threshold (flex angle)\n",
        "  for i, angle in enumerate(df['theta_flex_deg']):\n",
        "    if angle > bio_threshold_flex or angle < -bio_threshold_flex:\n",
        "      df.at[i, 'theta_flex_deg'] = np.nan\n",
        "    if angle <= lower_bound or angle >= upper_bound:\n",
        "      df.at[i, 'theta_flex'] = np.nan\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_angles(df):\n",
        "  \"\"\" Plots tail angle and body flexion time series.\n",
        "  Args: cleaned DataFrame with both tail angle (`theta_tail`) and body flexion (`theta_flex`).\n",
        "  Returns: plot of the tail angle and body flexion time series.\n",
        "  \"\"\"\n",
        "  # Plot setup\n",
        "  plt.figure(figsize=(10, 5))\n",
        "  plt.xlabel('Time (sec)')\n",
        "  plt.ylabel('Angle (degrees)')\n",
        "\n",
        "  # Plot filtered tail angle data (after cleaning)\n",
        "  plt.plot(df['index'] / 24, df['theta_head_deg'], color='green', label='Head Angle', marker='.', markersize=10, alpha=0.5, zorder=1)\n",
        "\n",
        "  # Plot filtered body flexion data (after cleaning)\n",
        "  plt.plot(df['index'] / 24, df['theta_flex_deg'], color='orange', label='Body Flexion Angle', marker='.', markersize=10, alpha=0.5, zorder=2)\n",
        "\n",
        "  # Display legend and plot\n",
        "  plt.legend()\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Collate and Export All Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [],
      "source": [
        "# paths for example data\n",
        "file_path = \"I:/documents/DLCPaper/revisions/data/kinematic/coordinates/all/07202023PANB0605DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv\"\n",
        "export_path = \"I:/documents/DLCPaper/revisions/data/kinematic/example_data/07202023PANB0605DLC_rawangles.csv\"\n",
        "\n",
        "# convert and export\n",
        "df_processed = process_csv(file_path)\n",
        "#df_frame_cleaned = frame_cleaner(df_processed)\n",
        "df_angles_added = add_angles(df_processed)\n",
        "#df_angles_cleaned = clean_angles(df_frame_cleaned)\n",
        "df_angles_added.to_csv(export_path, index=False)\n",
        "#df_angles_cleaned.to_csv(export_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 07202023PANB0202DLC\n",
            "KeyError: 'nose_x' - Problem in 07202023PANB0303DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "KeyError: 'nose_x' - Problem in 07202023PANB0301DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "KeyError: 'nose_x' - Problem in 07202023PANB0201DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "Processed: 07202023PANB0204DLC\n",
            "KeyError: 'nose_x' - Problem in 07202023PANB0304DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "KeyError: 'nose_x' - Problem in 07202023PANB0306DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "KeyError: 'nose_x' - Problem in 07202023PANB0505DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "KeyError: 'nose_x' - Problem in 07202023PANB0302DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "KeyError: 'nose_x' - Problem in 07202023PANB0501DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "Processed: 07202023PANB0604DLC\n",
            "Processed: 07202023PANB0503DLC\n",
            "Processed: 07202023PANB0701DLC\n",
            "KeyError: 'nose_x' - Problem in 07202023PANB0401DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "Processed: 07202023PANB0305DLC\n",
            "KeyError: 'nose_x' - Problem in 07202023PANB0705DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "Processed: 07202023PANB0605DLC\n",
            "Processed: 07202023PANB0502DLC\n",
            "Processed: 07202023PANB0702DLC\n",
            "Processed: 07202023PANB0504DLC\n",
            "KeyError: 'nose_x' - Problem in 07202023PANB0707DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "Processed: 07202023PANB0203DLC\n",
            "KeyError: 'nose_x' - Problem in 07252023PANB0102DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "KeyError: 'nose_x' - Problem in 07202023PANB0602DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "KeyError: 'nose_x' - Problem in 07202023PANB0802DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "Processed: 07202023PANB0804DLC\n",
            "KeyError: 'nose_x' - Problem in 07202023PANB0603DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "Processed: 07202023PANB0703DLC\n",
            "Processed: 07252023PANB0202DLC\n",
            "Processed: 07252023PANB0201DLC\n",
            "Processed: 07202023PANB0805DLC\n",
            "Processed: 07202023PANB0704DLC\n",
            "KeyError: 'nose_x' - Problem in 07202023PANB0601DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "Processed: 07202023PANB0706DLC\n",
            "Processed: 07202023PANB0801DLC\n",
            "Processed: 07252023PANB0101DLC\n",
            "Processed: 07252023PANB0203DLC\n",
            "Processed: 07252023PANB0303DLC\n",
            "Processed: 07252023PANB0302DLC\n",
            "Processed: 07252023PANB0304DLC\n",
            "Processed: 07252023PANB0301DLC\n",
            "Processed: 07252023PANB0204DLC\n",
            "KeyError: 'nose_x' - Problem in 07252023PANB0602DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "Processed: 07252023PANB0501DLC\n",
            "Processed: 07252023PANB0603DLC\n",
            "Processed: 07252023PANB0205DLC\n",
            "Processed: 07252023PANB0604DLC\n",
            "Processed: 07252023PANB0601DLC\n",
            "KeyError: 'nose_x' - Problem in 07252023PANB0703DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "Processed: 07252023PANB0704DLC\n",
            "Processed: 07252023PANB0605DLC\n",
            "KeyError: 'nose_x' - Problem in 07252023PANB0706DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "KeyError: 'nose_x' - Problem in 07252023PANB0707DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "Processed: 07252023PANB0708DLC\n",
            "Processed: 07252023PANB0701DLC\n",
            "Processed: 07252023PANB0705DLC\n",
            "Processed: 07252023PANB0702DLC\n",
            "Processed: 07252023PANB0709DLC\n",
            "KeyError: 'nose_x' - Problem in 07272023PANB0104DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "KeyError: 'nose_x' - Problem in 07272023PANB0101DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "KeyError: 'nose_x' - Problem in 07272023PANB0103DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "KeyError: 'nose_x' - Problem in 07272023PANB0105DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "KeyError: 'nose_x' - Problem in 07272023PANB0201DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "Processed: 07272023PANB0102DLC\n",
            "KeyError: 'nose_x' - Problem in 07272023PANB0106DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "KeyError: 'nose_x' - Problem in 07272023PANB0202DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "KeyError: 'nose_x' - Problem in 07272023PANB0301DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "KeyError: 'nose_x' - Problem in 07272023PANB0107DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "KeyError: 'nose_x' - Problem in 07272023PANB0203DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "KeyError: 'nose_x' - Problem in 07272023PANB0302DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "KeyError: 'nose_x' - Problem in 07272023PANB0204DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "Processed: 07272023PANB0401DLC\n",
            "Processed: 07272023PANB0502DLC\n",
            "KeyError: 'nose_x' - Problem in 07272023PANB0402DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "Processed: 07272023PANB0504DLC\n",
            "KeyError: 'nose_x' - Problem in 07272023PANB0601DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "KeyError: 'nose_x' - Problem in 07272023PANB0505DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "Processed: 07272023PANB0602DLC\n",
            "Processed: 07272023PANB0503DLC\n",
            "Processed: 07272023PANB0501DLC\n",
            "KeyError: 'nose_x' - Problem in 07272023PANB0605DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "KeyError: 'nose_x' - Problem in 07272023PANB0701DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "Processed: 07272023PANB0703DLC\n",
            "KeyError: 'nose_x' - Problem in 07272023PANB0802DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "Processed: 07272023PANB0604DLC\n",
            "Processed: 07272023PANB0801DLC\n",
            "Processed: 07272023PANB0803DLC\n",
            "Processed: 07272023PANB0702DLC\n",
            "Processed: 07272023PANB0804DLC\n",
            "Processed: 07272023PANB0805DLC\n",
            "KeyError: 'nose_x' - Problem in 07272023PANB0901DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "KeyError: 'nose_x' - Problem in 07272023PANB0902DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "KeyError: 'nose_x' - Problem in 07272023PANB1002DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "KeyError: 'nose_x' - Problem in 07272023PANB1004DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "KeyError: 'nose_x' - Problem in 07272023PANB0904DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "KeyError: 'nose_x' - Problem in 07272023PANB0903DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "KeyError: 'nose_x' - Problem in 07272023PANB1003DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "Processed: 08112022PANB0102DLC\n",
            "Processed: 07222022PANB0202DLC\n",
            "Processed: 08112022PANB0101 (2)DLC\n",
            "KeyError: 'nose_x' - Problem in 08112022PANB0301DLC_resnet50_jws_full_trainingFeb13shuffle1_200000.csv. Skipping this file.\n",
            "Processed: 08112022PANB0103DLC\n",
            "Processed: 08112022PANB0202DLC\n",
            "Processed: 08112022PANB0203DLC\n",
            "Processed: 08112022PANB0402DLC\n",
            "Processed: 08112022PANB0502DLC\n",
            "Processed: 08112022PANB0501DLC\n",
            "Processed: 08112022PANB0403DLC\n",
            "Processed: 08112022PANB0505DLC\n",
            "Processed: 08112022PANB0504DLC\n",
            "Processed: 08112022PANB0602DLC\n",
            "Processed: 08112022PANB0503DLC\n",
            "KeyError: 'nose_x' - Problem in 08112022PANB0603DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (1).csv. Skipping this file.\n",
            "Processed: 08112022PANB0604DLC\n",
            "Processed: 08112022PANB0401DLC\n",
            "Processed: 08222022PANB0203DLC\n",
            "Processed: 08222022PANB0201DLC\n",
            "Processed: 08112022PANB0605DLC\n",
            "Processed: 08222022PANB0301DLC\n",
            "Processed: 08222022PANB0702DLC\n",
            "Processed: 08222022PANB0303DLC\n",
            "Processed: 08302022PANB0202DLC\n",
            "KeyError: 'nose_x' - Problem in 08222022PANB0101DLC_resnet50_jws_full_trainingFeb13shuffle1_200000 (3).csv. Skipping this file.\n",
            "Processed: 08112022PANB0201DLC\n"
          ]
        }
      ],
      "source": [
        "# loops through and reads in files in folder of cleaned files from Tail_Position_Series_Constructor.ipynb\n",
        "folder_path = \"I:/documents/DLCPaper/revisions/data/kinematic/coordinates/all\"\n",
        "exportfolder_path = \"I:/documents/DLCPaper/revisions/data/kinematic/coordinates_transformed/all\"\n",
        "\n",
        "# get a list of all the files in the folder\n",
        "file_list = os.listdir(folder_path)\n",
        "\n",
        "# loop through CSV files\n",
        "for file_name in file_list:\n",
        "    if file_name.endswith(\".csv\") and not file_name.startswith(\"._\"):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        export_path = os.path.join(exportfolder_path, 'transf_' + file_name.split('_')[0] + '.csv')\n",
        "\n",
        "        try:\n",
        "            df_processed = process_csv(file_path)\n",
        "            df_frame_cleaned = frame_cleaner(df_processed)\n",
        "            df_angles_added = add_angles(df_frame_cleaned)\n",
        "            df_angles_cleaned = clean_angles(df_frame_cleaned)\n",
        "\n",
        "            df_angles_cleaned['file_name'] = file_name.split('_')[0]\n",
        "            df_angles_cleaned.to_csv(export_path, index=False)\n",
        "            print(f\"Processed: {file_name.split('_')[0]}\")\n",
        "\n",
        "        except KeyError as e:\n",
        "            print(f\"KeyError: {e} - Problem in {file_name}. Skipping this file.\")\n",
        "            continue  # Skip this file and move to the next one"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
